## Small-File Access in Parallel File Systems

原文链接：[Small-File Access in Parallel File Systems](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5161029)

摘要：现今的计算机科学需求创造了比以往更大的并行计算机，并且，存储系统也在发展，以满足这些需求。在这种场景下使用的并行文件系统正在被特殊化来获得对于大I/O操作的更高性能，当然，这是以牺牲其它可能的负载的情况下。虽然一些应用程序已经适应了I/O的最佳实践，在这些系统中可以获得很好的性能，许多应用程序最常见的I/O模式会产生许多小文件。当规模非常大时，当前的并行文件系统不能很好地为这些应用程序提供服务。

本文描述了超大规模系统中优化并行文件系统的小文件访问的五种技术。这五种技术都实现于一个并行文件系统(PVFS)，然后在两种平台上进行系统的测试。使用一个微基准测试和mdtest基准测试来测试一个无法预料的规模的优化。结果表明，在使用16384个核心的领先的计算平台上，与PVFS的基本配置相比，在小文件创建速率上提高905%，在小文件stat速率上提升1106%，

# 1 介绍

现今的计算机科学需求创造了比以往更大的并行计算机，并且，存储系统也在发展，以满足应用程序产生数据的速率。在这种场景下使用的并行文件系统正在被专用化，试图在计算科学应用负载方面通过底层存储硬件获得可能的最佳性能。这些专用的系统很大，常常进行排列以并发访问，并且一些应用已经发现，对几个GB的大文件访问，进行大量数据的访问是提升并行文件系统性能的最佳方式。其它一些应用继续使用其它的I/O策略，获得了不同方面的成功。与此同时，在一些新的领域的科学家开始使用高性能计算(HPC)资源来攻克他们专业领域的难题，这些新的应用提出了新的I/O需求。

通过对最近的工作负载的研究，可以得到一些结论。事实上，许多HPC存储系统不仅被用来存储大文件，还用来存储许多小文件。比如，2007年，国家能源研究科学计算中心对一个共享的并行文件系统的研究表明，该并行文件系统存储了1300万个文件，99%的文件大小小于64MB，43%的文件大小小于64KB。类似的，2007年，西北太平洋国家实验室的一项研究表明，在他们的系统中有1200万个文件，94%的文件大小小于64MB，58%的文件大小小于64KB。

更多的研究表明，这些文件可以来自许多地方，而不是一个行为异常的应用。几个科学领域，比如气候学、天文学和生物学，它们都会产生方便存储的数据集，并且在文件系统上作为独立文件进行组织。下面是每个领域产生的数据集的例子：

* 45万个社区气候系统模型文件，平均大小为61MB。
* 斯隆数字巡天计划产生了2000万张图片，平均大小小于1MB。
* 人类基因组产生了大约3000万个文件，平均大小为190KB。

在并行文件系统访问大量的小文件会将I/O的挑战从提供高I/O聚合带宽转向支持高并发的元数据访问速率。现在最常用的提升文件系统元数据速率的技术是客户端缓存。然而，在HPC系统中，有大量的多核处理器，结果，每个核心只有少量的RAM。这些系统的应用通常使用内存的大多数，只有很少的空间作缓存。此外，传统的保持一致性和故障恢复的技术并不是为这种大规模而设计的。

本文中，我们探究一种隐藏延迟、减少I/O和消息传送的策略，而且它不使用客户端的额外资源。我们陈述了五种并行文件系统中提升元数据并发访问和小文件I/O性能：server-driven file precreaton, the readdirplus POSIX extension, file stuffing, metadata commit coalescing, and eager data movement for reads and writes。在这五种技术中，前两种已经在并行文件系统实现中阐述过了。剩下的三种是众所周知的优化方式，但是，我们以一种新的方式将它们应用于并行文件系统环境。本文中，所有的五种技术都在一个文件系统(PVFS)中实现，然后在一致的环境中测试它们的相对值。我们还将分析用来评价IBM Blue Gene/P系统的前所未有的规模的行为。

本文按照下面的方式来组织。第二部分，我们陈述了PVFS的相关因素。第三部分，我们讨论我们的工作中用到的每个优化小文件I/O访问的技术。第四部分，我们在两种测试环境中使用微基准测试和人工负载在测试这些技术。第五部分，我们对相关工作进行总结。第六部分，我们对我们的发现进行总结，然后提供未来的一些工作的建议。

# 2 并行虚拟文件系统

并行虚拟文件系统项目是一个多机构共同合作的一个项目，该项目的目的是设计并实现一个为大规模HPC应用服务的并行文件系统。当前的PVFS实现通过消息抽象机制可以支持一系列网络，包括TCP/IP、InfiniBand、Protals、Myrinet MX。该系统广泛应用于研究工作，许多站点都部署了该系统，包括阿贡领导计算设施。它的部署细节在第四部分讨论。

## A PVFS服务器

PVFS文件系统由一个服务器集群组成，服务器提供元数据和数据的存储，可以用自定义的网络协议进行访问，和NFS3类似，自定义的网络协议是为了满足计算科学的需求而做的定制的改变。每个服务器管理它自己的本地存储空间，PVFS的商业版本将数据存储在本地目录树的文件中，而元数据存储在伯克利数据库中。这种方法消除了与块分配相关的通信开销，数据以对象的形式组织，这跟数据在对象存储设备上的存储类似。

当前版本的PVFS使用静态配置文件给指定的服务器分配元数据服务器(MDS)角色和I/O服务器(IOS)角色。PVFS通常会跨服务器划分对象句柄，句柄在PVFS文件系统上下文中是唯一的。PVFS通常会跨IOS条块化文件，但是不会存储冗余数据或者元数据的额外拷贝。对于容错配置，存储可能会附加上多个服务器，并且通过包来开启故障转移功能，比如Linux-HA心跳包。单独的目录存储在一个MDS上。目录保存了名字，和元数据对象相关的对象句柄，元数据对象可能跨越MDS。这种间接行为可以在数据放置方面提供强大的灵活性。

当部署在I/O密集型的应用时，PVFS文件系统可能会配置成一个MDS和多个IOS，然而，如果是通用型或者元数据密集型，所有的服务器既是MDS，又是IOS。本文中，在PVFS上的所有测试都配置为：所有的服务器既是MDS，又是IOS。

## B PVFS客户端

PVFS客户端通过API访问文件系统。Linux的VFS模块允许使用标准的POSIX的I/O引用程序接口，UNIX的实用程序和应用程序使用这些接口。通过称为PVFS系统接口的方式，用户空间的库提供文件访问，更高级的库在访问PVFS文件时旁路掉内核，比如MPI-IO库。

不管使用何种应用程序接口，最终，PVFS客户端在访问对象时，首先通过lookup操作将路径名映射到对象句柄。lookup之后是getattr，getattr操作请求文件的统计信息和文件的分布。PVFS文件的分布情况包括该文件中一系列保存数据的对象和一个函数，该函数可以将文件位置映射到分布对象中的位置，类似于pNFS中的layout。一旦文件创建，文件分布就不会改变(除了文件装满了)，因此，客户端可能会永远缓存文件分布。由于这个原因，一旦客户端获取文件分布，它们就可以直接与IOS通信进行接下来的读写操作。

PVFS客户端分别为lookup和getattr操作设置两个缓存，一个是名字空间缓存，一个是属性缓存。这些缓存的主要目的是弥补Linux内核的VFS生成的元数据访问模式。VFS通常不会在访问一个文件时对同一个文件执行多个stat和路径lookup操作。本文中的实验将属性缓存和名字空间缓存超时设置为100ms。这对隐藏重复的lookup和getattr操作是足够的，并且不会在客户端之间产生过多的stat。概念上讲，这与Ceph文件系统采用的readdir_ttl和stat_ttl超时很类似。

# 3 PVFS中优化小文件访问

由于在小文件访问过程中，客户端与服务器之间的数据传送量很少，性能通常取决于一些单独的网络和I/O操作，当然，I/O操作是执行所必须的，以及这些操作从客户端隐藏的延迟度。在我们的工作中，我们采用隐藏延迟、减少消息和I/O的策略，而不使用额外的资源或者客户端之间进行额外的一致性要求。

这部分描述了五种优化方案，它们针对小文件访问的多种方面。其中一些优化方案已经出现在PVFS产品发行版中，但是在这方面的工作是第一次出现，其它的方案还处在原型阶段。

一个明显的问题是：什么是小文件。在我们的案例中，小文件是那些不需要进行条带化的文件；处理小文件的创建、删除和信息收集的优化方案对于几MB的文件也是适用的。在我们的工作中，小文件访问能够随着控制消适应消息缓冲区(当前PVFS使用的是16KB)。

## A 预创建对象

PVFS中的文件创建是被客户端创建文件操作驱动的多步骤过程。客户端与对象将要放置的服务器进行通信，首先创建一个对象存储元数据，再创建一个对象集存储数据。元数据对象是被一个单独的操作更新，元数据对象保存了数据对象的列表和一个描述文件位置如何映射到数据对象的区域的分布函数。之后在合适的服务器上添加一个目录项。如果发生了错误，客户端有责任清除添加失败的对象。如果客户端在创建过程中发生故障，对象可能就是孤儿对象，但是名字空间还是完整的。





