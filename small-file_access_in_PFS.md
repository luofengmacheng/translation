## Small-File Access in Parallel File Systems

原文链接：[Small-File Access in Parallel File Systems](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5161029)

摘要：现今的计算机科学需求创造了比以往更大的并行计算机，并且，存储系统也在发展，以满足这些需求。在这种场景下使用的并行文件系统正在被特殊化来获得对于大I/O操作的更高性能，当然，这是以牺牲其它可能的负载的情况下。虽然一些应用程序已经适应了I/O的最佳实践，在这些系统中可以获得很好的性能，许多应用程序最常见的I/O模式会产生许多小文件。当规模非常大时，当前的并行文件系统不能很好地为这些应用程序提供服务。

本文描述了超大规模系统中优化并行文件系统的小文件访问的五种技术。这五种技术都实现于一个并行文件系统(PVFS)，然后在两种平台上进行系统的测试。使用一个微基准测试和mdtest基准测试来测试一个无法预料的规模的优化。结果表明，在使用16384个核心的领先的计算平台上，与PVFS的基本配置相比，在小文件创建速率上提高905%，在小文件stat速率上提升1106%，

# 1 介绍

现今的计算机科学需求创造了比以往更大的并行计算机，并且，存储系统也在发展，以满足应用程序产生数据的速率。在这种场景下使用的并行文件系统正在被专用化，试图在计算科学应用负载方面通过底层存储硬件获得可能的最佳性能。这些专用的系统很大，常常进行排列以并发访问，并且一些应用已经发现，对几个GB的大文件访问，进行大量数据的访问是提升并行文件系统性能的最佳方式。其它一些应用继续使用其它的I/O策略，获得了不同方面的成功。与此同时，在一些新的领域的科学家开始使用高性能计算(HPC)资源来攻克他们专业领域的难题，这些新的应用提出了新的I/O需求。

通过对最近的工作负载的研究，可以得到一些结论。事实上，许多HPC存储系统不仅被用来存储大文件，还用来存储许多小文件。比如，2007年，国家能源研究科学计算中心对一个共享的并行文件系统的研究表明，该并行文件系统存储了1300万个文件，99%的文件大小小于64MB，43%的文件大小小于64KB。类似的，2007年，西北太平洋国家实验室的一项研究表明，在他们的系统中有1200万个文件，94%的文件大小小于64MB，58%的文件大小小于64KB。

更多的研究表明，这些文件可以来自许多地方，而不是一个行为异常的应用。几个科学领域，比如气候学、天文学和生物学，它们都会产生方便存储的数据集，并且在文件系统上作为独立文件进行组织。下面是每个领域产生的数据集的例子：

* 45万个社区气候系统模型文件，平均大小为61MB。
* 斯隆数字巡天计划产生了2000万张图片，平均大小小于1MB。
* 人类基因组产生了大约3000万个文件，平均大小为190KB。

在并行文件系统访问大量的小文件会将I/O的挑战从提供高I/O聚合带宽转向支持高并发的元数据访问速率。现在最常用的提升文件系统元数据速率的技术是客户端缓存。然而，在HPC系统中，有大量的多核处理器，结果，每个核心只有少量的RAM。这些系统的应用通常使用内存的大多数，只有很少的空间作缓存。此外，传统的保持一致性和故障恢复的技术并不是为这种大规模而设计的。

本文中，我们探究一种隐藏延迟、减少I/O和消息传送的策略，而且它不使用客户端的额外资源。我们陈述了五种并行文件系统中提升元数据并发访问和小文件I/O性能：server-driven file precreaton, the readdirplus POSIX extension, file stuffing, metadata commit coalescing, and eager data movement for reads and writes。在这五种技术中，前两种已经在并行文件系统实现中阐述过了。剩下的三种是众所周知的优化方式，但是，我们以一种新的方式将它们应用于并行文件系统环境。本文中，所有的五种技术都在一个文件系统(PVFS)中实现，然后在一致的环境中测试它们的相对值。我们还将分析用来评价IBM Blue Gene/P系统的前所未有的规模的行为。

本文按照下面的方式来组织。第二部分，我们陈述了PVFS的相关因素。第三部分，我们讨论我们的工作中用到的每个优化小文件I/O访问的技术。第四部分，我们在两种测试环境中使用微基准测试和人工负载在测试这些技术。第五部分，我们对相关工作进行总结。第六部分，我们对我们的发现进行总结，然后提供未来的一些工作的建议。

# 2 并行虚拟文件系统

并行虚拟文件系统项目是一个多机构共同合作的一个项目，该项目的目的是设计并实现一个为大规模HPC应用服务的并行文件系统。当前的PVFS实现通过消息抽象机制可以支持一系列网络，包括TCP/IP、InfiniBand、Protals、Myrinet MX。该系统广泛应用于研究工作，许多站点都部署了该系统，包括阿贡领导计算设施。它的部署细节在第四部分讨论。

## A PVFS服务器

PVFS文件系统由一个服务器集群组成，服务器提供元数据和数据的存储，可以用自定义的网络协议进行访问，和NFS3类似，自定义的网络协议是为了满足计算科学的需求而做的定制的改变。每个服务器管理它自己的本地存储空间，PVFS的商业版本将数据存储在本地目录树的文件中，而元数据存储在伯克利数据库中。这种方法消除了与块分配相关的通信开销，数据以对象的形式组织，这跟数据在对象存储设备上的存储类似。

当前版本的PVFS使用静态配置文件给指定的服务器分配元数据服务器(MDS)角色和I/O服务器(IOS)角色。PVFS通常会跨服务器划分对象句柄，句柄在PVFS文件系统上下文中是唯一的。PVFS通常会跨IOS条块化文件，但是不会存储冗余数据或者元数据的额外拷贝。对于容错配置，存储可能会附加上多个服务器，并且通过包来开启故障转移功能，比如Linux-HA心跳包。单独的目录存储在一个MDS上。目录保存了名字，和元数据对象相关的对象句柄，元数据对象可能跨越MDS。这种间接行为可以在数据放置方面提供强大的灵活性。

当部署在I/O密集型的应用时，PVFS文件系统可能会配置成一个MDS和多个IOS，然而，如果是通用型或者元数据密集型，所有的服务器既是MDS，又是IOS。本文中，在PVFS上的所有测试都配置为：所有的服务器既是MDS，又是IOS。









